# Expectation Maximization (EM)
## _in progress_
EM is an algorithm for finding the maximum likelihood parameters $\theta$ for probabilistic models having latent variables. EM consists of two steps E (Expectation) and M (Maximization). EM is a special case of a [MM algorithm](https://en.wikipedia.org/wiki/MM_algorithm) for optimization.

Suppose we want to find the maximum likelihood for a probablistic model with latent variables. The set of observed variables is given denote by $\mathbf{x}$ while the set of latent variables is given by $\mathbf{z}$. Suppose the joint distribution is parameterized by $\theta$, $p(\mathbf{x}, \mathbf{z}\|\theta)$. Suppose we need to maximize the likelihood of the observed data; therefore, we seek find the parameters $\theta$ that maximize the likelihood of the marignal distribution $p(\mathbf{x}\|\theta)$. Although optional, it is common practice to use the log-likelihood.
The log likelihood function of the observed variables is given by 

$$\log p(\mathbf{x}|\theta) = \log \sum_{\mathbf{z}} p(\mathbf{x}, \mathbf{z}|\theta)$$

The underlying assumpition is that $p(\mathbf{x},\mathbf{z}\|\theta)$ 
yields an easier optimization problem then $p(\mathbf{x}\|\theta)$. For any choice of distribution $q$ over $\mathbf{z}$, the following holds:

$$\log p(\mathbf{x}|\theta) = \mathcal{L}(q,\theta) + \text{KL}(q||p)$$

where 

$$ \mathcal{L}(q, \theta) = \sum_{\mathbf{z}} q(\mathbf{z}) \log \frac{p(\mathbf{x},\mathbf{z}|\theta)}{q(\mathbf{z})}$$

$$ \text{KL}(q||p) = -\sum_{\mathbf{z}} q(\mathbf{z}) \log \frac{p(\mathbf{z}|\mathbf{x},\theta)}{q(\mathbf{z})}$$

<details>
  <summary>Proof</summary>
  
$$ \mathcal{L}(q, \theta) = \sum_{\mathbf{z}} q(\mathbf{z}) [\log p(\mathbf{x},\mathbf{z}|\theta) - \log q(\mathbf{z})]$$
$$ \mathcal{L}(q, \theta) = \sum_{\mathbf{z}} q(\mathbf{z}) [\log p(\mathbf{z}|\mathbf{x},\theta) + \log p(\mathbf{x}) - \log q(\mathbf{z})]$$
$$ \mathcal{L}(q, \theta) = \sum_{\mathbf{z}} q(\mathbf{z}) [\log p(\mathbf{z}|\mathbf{x},\theta) - \log q(\mathbf{z})] + \log p(\mathbf{x}) $$

$$ \mathcal{L}(q, \theta) = -\text{KL}(q||p) + \log p(\mathbf{x}) $$ 

</details>

Since $\text{KL}(q\|\|p) \ge 0$, then it follows that $\mathcal{L}(q,\theta) \le \log p(\mathbf{x})$. Therefore $\mathcal{L}(q,\theta)$ is a lower bound to $\log p(\mathbf{x})$. 

Suppose the current parameters are denoted by $\theta^{\text{old}}$. In E-step, the lower bound $\mathcal{L}(q,\theta^\text{old})$ is maximized with respect to $q$ with $\theta^\text{old}$ fixed. The tightest lower bound $\mathcal{L}(q,\theta^\text{old})$ occurs when $q(\mathbf{z}) = p(\mathbf{z}\|\mathbf{x},\theta^\text{old})$ since the KL divergence term disappears and as a result, $\mathcal{L}(q,\theta^\text{old}) = \log p(\mathbf{x} \| \theta^{\text{old}})$. Note for $q(\mathbf{z}) = p(\mathbf{z}\|\mathbf{x},\theta^\text{old})$, $\mathcal{L}(q, \theta) \le \log p(\mathbf{x}\| \theta)$ for $\theta \neq \theta^{\text{old}}$ but achieves equality for $\theta = \theta^\text{old}$. $\mathcal{L}(p(\mathbf{z}\|\mathbf{x},\theta^\text{old}), \theta)$ can be seen as a minorizing surrogate function for $\log p(\mathbf{x}|\theta)$.

In the M-step, the distribution $q(\mathbf{z}) = p(\mathbf{z}\|\mathbf{x},\theta^\text{old})$  is held fixed but the lower bound is maximized with respect to the parameters $\theta$. The updated parameters are denoted by $\theta^\text{new}$. Recall that $\mathcal{L}(q, \theta^{\text{old}}) = \log p(\mathbf{x} \| \theta^{\text{old}})$, $\log p(\mathbf{x} \| \theta^{\text{old}}) \le \mathcal{L}(q, \theta^\text{new})$. Since $q(\mathbf{z}) = p(\mathbf{z}\|\mathbf{x},\theta^\text{old})$, $\mathcal{L}(q, \theta^\text{new}) \le \log p(\mathbf{x}\|\theta^\text{new})$ and therefore $\log p(\mathbf{x} \| \theta^{\text{old}}) \le \log p(\mathbf{x} \| \theta^{\text{new}})$. In next E-step, $q(\mathbf{z})$ will be set to $p(\mathbf{z}\|\mathbf{x},\theta^\text{new})$ thus re-establishing equality, and in the following M-step, the new parameters will increase or maintain the same likelihood.

In the E-step, if we set $q(\mathbf{z}) = p(\mathbf{z}\|\mathbf{x},\theta^\text{old})$

$$ \mathcal{L}(q, \theta) = \sum_{\mathbf{z}}p(\mathbf{z}|\mathbf{x},\theta^\text{old}) \log \frac{p(\mathbf{x},\mathbf{z}|\theta)}{p(\mathbf{z}|\mathbf{x},\theta^\text{old})}$$

$$ \mathcal{L}(q, \theta) = \sum_{\mathbf{z}}p(\mathbf{z}|\mathbf{x},\theta^\text{old}) [\log p(\mathbf{x},\mathbf{z}|\theta) - \log p(\mathbf{z}|\mathbf{x},\theta^\text{old})]$$

Since the second term does not rely on $\theta$ during the M-step it is considered a constant and as a result, the M-step effectively optimizes $Q$:

$$ \mathcal{L}(q, \theta) = \sum_{\mathbf{z}}p(\mathbf{z}|\mathbf{x},\theta^\text{old}) \log p(\mathbf{x},\mathbf{z}|\theta) + \text
{constant}$$

$$Q(\theta, \theta^{\text{old}}) = \sum_{\mathbf{z}}p(\mathbf{z}|\mathbf{x},\theta^\text{old}) \log p(\mathbf{x},\mathbf{z}|\theta) = \mathbb{E}_{p(\mathbf{z}|\mathbf{x},\theta^\text{old})}[\log p(\mathbf{x},\mathbf{z}|\theta) ]$$

The quantity in the expectation is the log likelihood of the complete distribution, and in order to evaluate $Q$ we need to evaluate $p(\mathbf{z}\|\mathbf{x},\theta^\text{old})$. Typically the primary focus of the E-step is to evaluate $p(\mathbf{z}\|\mathbf{x},\theta^\text{old})$.

**EM Algorithm:**

1. Initialize $\theta^{\text{old}}$

2. **E-Step:** Evaluate $p(\mathbf{z}\|\mathbf{x},\theta^\text{old})$.

3. **M-Step:** $\theta^{\text{new}} = \text{argmax}_\theta Q(\theta, \theta^{\text{old}})$

4. If not converged, $\theta^{\text{old}} \gets \theta^{\text{new}}$ and go to step 2.

## Toy Example

Our dataset is generated using the following Gaussian mixture:

$$p(x|\mu) = \frac{1}{2} \mathcal{N}(x|2, 1) + \frac{1}{2} \mathcal{N}(x|-2,1)$$ 

and consists of $1000$ samples.

Suppose we have a Gaussian Mixture that is parameterized as such

$$p(x|\mu) = \frac{1}{2} \mathcal{N}(x|\mu, 1) + \frac{1}{2} \mathcal{N}(x|-\mu, 1)$$ 


The $\mu_\text{old} = .5$ for computing the lower bound

<figure>
<img src="{{site.baseurl}}/images/post_im/em/plot.png">
  <figcaption>Log Marginal Likelihood and Lower bound</figcaption>
</figure>

<a href="https://colab.research.google.com/github/ceroytres/website_notebooks/blob/master/EM.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a>

Note that the lower bound achieves equality with the log margnial likelihood at $\mu_\text{old}$  and it is convex unlike the log marginal likelihood

### References

[1] Bishop, Christopher M. Pattern Recognition and Machine Learning. Springer, 2006.