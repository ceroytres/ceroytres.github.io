# Policy Iteration as Expectation Maximization

## Expectation Maximization (EM)

EM is an algorithm for finding the maximum likelihood parameters $\theta$ for probabilistic models having latent variables. EM consists of two steps E (Expectation) and M (Maximization). EM is a special case of a [MM algorithm](https://en.wikipedia.org/wiki/MM_algorithm) for optimization.

Suppose we want to find the maximum likelihood for a probablistic model with latent variables. The set of observed variables is given denote by $\mathbf{x}$ while the set of latent variables is given by $\mathbf{z}$. Suppose the joint distribution is parameterized by $\theta$, $p(\mathbf{x}, \mathbf{z}\|\theta)$. The log likelihood function of the observed variables is given by 

$$\log p(\mathbf{x}|\theta) = \log \sum_{\mathbf{z}} p(\mathbf{x}, \mathbf{z}|\theta)$$

The underlying assumpition is that $p(\mathbf{x},\mathbf{z}\|\theta)$ 
yields an easier optimization problem then $p(\mathbf{x}\|\theta)$. For any choice of distribution $q$ over $\mathbf{z}$, the following holds:

$$\log p(\mathbf{x}|\theta) = \mathcal{L}(q,\theta) + \text{KL}(q||p)$$

where 

$$ \mathcal{L}(q, \theta) = \sum_{\mathbf{z}} q(\mathbf{z}) \log \frac{p(\mathbf{x},\mathbf{z}|\theta)}{q(\mathbf{z})}$$

$$ \text{KL}(q||p) = -\sum_{\mathbf{z}} q(\mathbf{z}) \log \frac{p(\mathbf{z}|\mathbf{x},\theta)}{q(\mathbf{z})}$$

<details>
  <summary>Proof</summary>
  
$$ \mathcal{L}(q, \theta) = \sum_{\mathbf{z}} q(\mathbf{z}) [\log p(\mathbf{x},\mathbf{z}|\theta) - \log q(\mathbf{z})]$$
$$ \mathcal{L}(q, \theta) = \sum_{\mathbf{z}} q(\mathbf{z}) [\log p(\mathbf{z}|\mathbf{x},\theta) + \log p(\mathbf{x}) - \log q(\mathbf{z})]$$
$$ \mathcal{L}(q, \theta) = \sum_{\mathbf{z}} q(\mathbf{z}) [\log p(\mathbf{z}|\mathbf{x},\theta) - \log q(\mathbf{z})] + \log p(\mathbf{x}) $$

$$ \mathcal{L}(q, \theta) = -\text{KL}(q||p) + \log p(\mathbf{x}) $$ 

</details>

Since $\text{KL}(q\|\|p) \ge 0$, then it follows that $\mathcal{L}(q,\theta) \le \log p(\mathbf{x})$. Therefore $\mathcal{L}(q,\theta)$ is a lower bound to $\log p(\mathbf{x})$. 

Suppose the current parameter are denoted by $\theta^{\text{old}}$. In E-step, the lower bound $\mathcal{L}(q,\theta^\text{old})$ is maximized with respect to $q$ with $\theta^\text{old}$ fixed. The tightest lower bound $\mathcal{L}(q,\theta^\text{old})$ occurs when $q(\mathbf{z}) = p(\mathbf{z}\|\mathbf{x},\theta^\text{old})$ since the KL divergence term disappears and as a result, $\mathcal{L}(q,\theta^\text{old}) = \log p(\mathbf{x} \| \theta^{\text{old}})$. Note for $q(\mathbf{z}) = p(\mathbf{z}\|\mathbf{x},\theta^\text{old})$, $\mathcal{L}(q, \theta) \le \log p(\mathbf{x}\| \theta)$ for $\theta \neq \theta^{\text{old}}$ but achieves equality for $\theta = \theta^\text{old}$. $\mathcal{L}(p(\mathbf{z}\|\mathbf{x},\theta^\text{old}), \theta)$ can be seen a minorizing surrogate function.

In the M-step, the distribution $q(\mathbf{z}) = p(\mathbf{z}\|\mathbf{x},\theta^\text{old})$  is held fixed but the lower bound is maximized with respect to the parameters $\theta$. The updated parameters are denoted by $\theta^\text{new}$. Recall that $\mathcal{L}(q, \theta^{\text{old}}) = \log p(\mathbf{x} \| \theta^{\text{old}})$, $\log p(\mathbf{x} \| \theta^{\text{old}}) \le \mathcal{L}(q, \theta^\text{new})$. Since $q(\mathbf{z}) = p(\mathbf{z}\|\mathbf{x},\theta^\text{old})$, $\mathcal{L}(q, \theta^\text{new}) \le \log p(\mathbf{x}\|\theta^\text{new})$ and therefore $\log p(\mathbf{x} \| \theta^{\text{old}}) \le \log p(\mathbf{x} \| \theta^{\text{new}})$. In next E-step, $q(\mathbf{z})$ will be set to $p(\mathbf{z}\|\mathbf{x},\theta^\text{new})$ thus re-establishing equality, and in the following M-step, the new parameters will increase or maintain the same likelihood.

In the E-step, if we set $q(\mathbf{z}) = p(\mathbf{z}\|\mathbf{x},\theta^\text{old})$

$$ \mathcal{L}(q, \theta) = \sum_{\mathbf{z}}p(\mathbf{z}|\mathbf{x},\theta^\text{old}) \log \frac{p(\mathbf{x},\mathbf{z}|\theta)}{p(\mathbf{z}|\mathbf{x},\theta^\text{old})}$$

$$ \mathcal{L}(q, \theta) = \sum_{\mathbf{z}}p(\mathbf{z}|\mathbf{x},\theta^\text{old}) [\log p(\mathbf{x},\mathbf{z}|\theta) - \log p(\mathbf{z}|\mathbf{x},\theta^\text{old})]$$

Since the second term does not rely on $\theta$ during the M-step it is considered a constant and as a result, the M-step effectively optimizes $Q$:

$$ \mathcal{L}(q, \theta) = \sum_{\mathbf{z}}p(\mathbf{z}|\mathbf{x},\theta^\text{old}) \log p(\mathbf{x},\mathbf{z}|\theta) + \text
{constant}$$

$$Q(\theta, \theta^{\text{old}}) = \sum_{\mathbf{z}}p(\mathbf{z}|\mathbf{x},\theta^\text{old}) \log p(\mathbf{x},\mathbf{z}|\theta) = \mathbb{E}_{p(\mathbf{z}|\mathbf{x},\theta^\text{old})}[\log p(\mathbf{x},\mathbf{z}|\theta) ]$$

The quantity in the expectation is the log likelihood of the complete distribution, and in order to evaluate $Q$ we need to evaluate $p(\mathbf{z}\|\mathbf{x},\theta^\text{old})$. Typically the primary focus of the E-step is to evaluate $p(\mathbf{z}\|\mathbf{x},\theta^\text{old})$.

**EM Algorithm:**

1. Initialize $\theta^{\text{old}}$

2. **E-Step:** Evaluate $p(\mathbf{z}\|\mathbf{x},\theta^\text{old})$.

3. **M-Step:** $\theta^{\text{new}} = \text{argmax}_\theta Q(\theta, \theta^{\text{old}})$

4. If not converged, $\theta^{\text{old}} \gets \theta^{\text{new}}$ and go to step 2.





## Markov Decision Process

A Markov Decision Process (MDP) consist of the following:
* Initial State Distribution $p(s_0)$
* State Transition Probability $p(s_{t+1}=s'\|s_t=s, a_t=a)$
* Reward Probability $p(r_{t+1} = r\|s_t, a_t)$
* Policy $ \pi(a\|s) = P(a_t = a \| s_t = s)$
* Discounting factor $\gamma \in [0,1)$ 




[1] Toussaint, Marc, Amos Storkey, and Stefan Harmeling. "Expectation-Maximization methods for solving (PO) MDPs and optimal control problems." Inference and Learning in Dynamic Models (2010).
[2] Bishop, Christopher M. Pattern Recognition and Machine Learning. Springer, 2006.