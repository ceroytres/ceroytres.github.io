# Generative Adversarial Networks (GANs)

The following contains my notes for my deep dive into the math behind simple GANs.

# Prelimiaries and Definitions


* __Generative Model__:  Given a training set sampled from a distribution $p\_{\text{data}}(x)$, a generative model attempts to estimate $p\_{\text{data}}(x)$. The distribution of the model is denoted by $p\_{\text{model}}$ 
* __Minimax Game__:
* __Convex set__: A set $\mathcal{S}$ is a convex set if for any two members $x_1, x_2 \in \mathcal{S}$ and scalar $\alpha \in [0,1]$, the convex combination defined by $\alpha x_1 + (1-\alpha) x_2 \in S$.  
* __Convex function__: If $f$ is a convex function, then for all $\alpha \in [0,1]$ and $x_1, x_2 \in \mathcal{S}$, where $\mathcal{S}$ is a convex set, the following is true $f(\alpha x_1 + (1-\alpha) x_2) \leq \alpha f(x_1) + (1-\alpha) f(x_2)$.
* __Iterated Expectation__: For two random variables $X,Y$ , the following holds $\mathbb{E}\[g(X,Y)\] = \mathbb{E}\_{Y}\[ \mathbb{E}\_\{X\|Y\} \[g(X,Y)\] \]$ 
* __Kullback–Leibler (KL) divergence__: Measures the difference between two distributions $p(x)$ and $q(x)$. It is computed as follows: $$ \text{KL}(p||q) = - \int p(x) \log\bigg(\frac{q(x)}{p(x)}\bigg) dx = - \mathbb{E}_{p(x)} \log\bigg(\frac{q(x)}{p(x)}\bigg) $$.
The KL divergence is not symmetric $\text\{KL}\(p\|\|q) \neq \text\{KL\}(q\|\|p)$ and the measure is non-negative $\text{KL}(p\|\|q) \geq 0$ and it achieves equality if and only if $p(x) = q(x)$. The KL divergence is not considered a metric in the traditional sense. 


Generative model follow to fit into two categories: Explicit and Implicit

<figure>
<img src="{{site.baseurl}}/images/post_im/model_tax.png">
  <figcaption>Generative Model Fitting Taxonomy from [2]</figcaption>
</figure>


## Maximum Likelihood Estimation (MLE) using an Explicit Density

Many generative models are trained using MLE with an explicit density function.  In particular, the model distribution $p\_\{\text{model}\}$ is selected from a specific parametric family of distributions with parameters $\theta$ (e.g. Gaussian, Bernoulli). Given a model distribution $p\_\{\text{model}\}\(x;\theta\)$ with parameters $\theta$ and an i.i.d dataset $\\{x_i\\}\_{i=1}^N$ sampled from $p\_{\text{data}}(x)$. The likelihood of the data under the given model is given by $\prod\_\{i=1\}^N p\_\{\text{model}\}(x\_i;\theta)$. ML estimation involves finding the parameters that maximize the likelihood of the data under the given model:

$$ \theta^* = \text{argmax}_\theta \prod_{i=1}^N p_{\text{model}}(x_i;\theta) $$

Since $0<a < b \implies \log a < \log b$,

$$ \theta^* = \text{argmax}_\theta \sum_{i=1}^N \log p_{\text{model}}(x_i;\theta)$$

Maximizing the ML is equivalent to minimizing the KL divergence between the $p\_{\text{data}}(x)$ and $p\_\{\text{model}\}\(x;\theta\)$:

$$ \text{argmax}_\theta \sum_{i=1}^N \log p_{\text{model}}(x_i;\theta)$$

$$ \text{argmin}_\theta -\frac{1}{N} \sum_{i=1}^N \log p_{\text{model}}(x_i;\theta)$$

$$ \text{argmin}_\theta -\frac{1}{N} \sum_{i=1}^N [\log p_{\text{model}}(x_i;\theta)  - \log p_{\text{data}}(x)]$$

Using the law of large numbers:

$$ \text{argmin}_\theta -\mathbb{E}_{p_{\text{data}}(x)}[\log p_{\text{model}}(x_i;\theta)  - \log p_{\text{data}}(x)]$$

$$ \text{argmin}_\theta -\mathbb{E}_{p_{\text{data}}(x)}[\log \frac{ p_{\text{model}}(x_i;\theta)}{ p_{\text{data}}(x)}]$$

In essence, the MLE attempts to find the distribution in the parametric family $p_{\text{model}}(x;\theta)$ that is the closest to the $p\_{\text{data}}(x)$. As suggested in [2], sometimes tractable explicit densities impose design limitations, so explicit models with intractablities are used instead. Explicit models with intractablities often require variational approximations or Markov chain approximations.

GAN are not trained using MLE. Instead, GANs belong to a family of generative models that aren't fitted using explicit ML, and such models are called implict models.

## Implict models

An implict model samples from a latent variable distribution $p(z)$ where $z \in \mathcal{Z}$, and uses a deterministic function $G: \mathcal{Z} \mapsto \mathcal{X}$, in order to map the latent variable $z$ to $x$. The generation process is shown below as graphical model:

<figure>
<img src="{{site.baseurl}}/images/post_im/implicit.png">
  <figcaption>Implicit Model as a Graphical Model</figcaption>
</figure>

Typically, the function $G$ is parameterized with a set of parameters $\theta$.In the deep learning setting, $G_\theta$ is a neural network model.


# GANs

### Setup


### Training


# Evaluation Metrics

Evalution metrics are still an open research problem. 

### Parzen Density Estimation

Parzen Density Estimation using a Gaussian Kernel with bandwidth $\sigma$:

$$p_n(x) = \frac{1}{n}\sum_{i=1}^n \frac{1}{(2\pi\sigma^2)^{d/2}} \exp{\frac{1}{2\sigma^2}||x-x_i||^2}$$

where $x_i \in \mathbb{R}^d \sim p_g$ and $\sigma$ is determined using cross-validation. 
* The Parzen Density estimate attempts to estimate $p_g$. 
* The Parzen Density estimate is used to evalute the likelihood of the test data under the estimated $p_g$. 
* Parzen density estimates tend to perform poorly in high dimensional spaces.

### Inception Score (IS)

$$ \text{IS} = \exp(\mathbb{E}_{x\sim p_g} KL(p(y|x)||p(y)) $$

Both $p(y\|x)$ and $p(y)= \int p(y\|x)p\_g(x) dx \approx \frac\{1\}\{n\}\sum\_i p(y\|x\_i)$ are estimated using an ImageNet pre-trained Inception v3. 

* $p(y\|x)$ should be low entropy if the objects produce a clear and yield confident classifications.
* $p(y)$ generated samples should have high diversity and therefore high entropy.

A higher IS implies better generative quality. IS has some drawbacks such a sensitivity to the Inception weights used, fails to detect memorization, and the need to estimate $p(y)$. See [10] for a detailed discussion on the IS.  

# References:
1. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, BingXu, David Warde-Farley, Sherjil Ozair, Aaron Courville,and Yoshua Bengio,Generative adversarial nets, Advances in neural information processing systems, 2014,pp. 2672–2680
2. Ian Goodfellow,Nips 2016 tutorial:  Generative adversarialnetworks, arXiv preprint arXiv:1701.00160 (2016)
2. Mohamed, Shakir, and Balaji Lakshminarayanan. "Learning in implicit generative models." arXiv preprint arXiv:1610.03483 (2016).
3. [Pieter Abbeel's CS294-158-SP20 Deep Unsupervised Learning Spring 2020 Notes](https://drive.google.com/open?id=1qCVpu2zFz1uEe3QcNHGlaT1Rs2u8HrCc)
2. Rockafellar, R. T. "Convex analysis in the calculus of variations." Advances in Convex Analysis and Global Optimization. Springer, Boston, MA, 2001. 135-151.
2. Salimans, Tim, et al. "Improved techniques for training gans." Advances in neural information processing systems. 2016.
10. Barratt, Shane, and Rishi Sharma. "A note on the inception score." arXiv preprint arXiv:1801.01973 (2018).