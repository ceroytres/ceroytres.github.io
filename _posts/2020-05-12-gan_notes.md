# Generative Adversarial Networks (GANs)

# ___Under Construction___

The following contains my notes for my deep dive into the math behind simple GANs. GAN were proposed in the following [paper](https://arxiv.org/abs/1406.2661)[1]. 

# Prelimiaries and Definitions


* __Generative Model__:  Given a training set sampled from a distribution $p\_{\text{data}}(x)$, a generative model attempts to estimate $p\_{\text{data}}(x)$. The distribution of the model is denoted by $p\_{\text{model}}$ 
* __Minimax Game__:  Focuses on minimizing a player's loss in the maximum loss situation.
* __Convex set__: A set $\mathcal{S}$ is a convex set if for any two members $x_1, x_2 \in \mathcal{S}$ and scalar $\alpha \in [0,1]$, the convex combination defined by $\alpha x_1 + (1-\alpha) x_2 \in S$.  
* __Convex function__: If $f$ is a convex function, then for all $\alpha \in [0,1]$ and $x_1, x_2 \in \mathcal{S}$, where $\mathcal{S}$ is a convex set, the following is true $f(\alpha x_1 + (1-\alpha) x_2) \leq \alpha f(x_1) + (1-\alpha) f(x_2)$.
* __Iterated Expectation__: For two random variables $X,Y$ , the following holds $\mathbb{E}\[g(X,Y)\] = \mathbb{E}\_{Y}\[ \mathbb{E}\_\{X\|Y\} \[g(X,Y)\] \]$ 
* __Kullback–Leibler (KL) divergence__: Measures the difference between two distributions $p(x)$ and $q(x)$. It is computed as follows: $$ \text{KL}(p||q) = - \int p(x) \log\bigg(\frac{q(x)}{p(x)}\bigg) dx = - \mathbb{E}_{p(x)} \log\bigg(\frac{q(x)}{p(x)}\bigg) $$.
The KL divergence is not symmetric $\text\{KL}\(p\|\|q) \neq \text\{KL\}(q\|\|p)$ and the measure is non-negative $\text{KL}(p\|\|q) \geq 0$ and it achieves equality if and only if $p(x) = q(x)$. The KL divergence is not considered a metric in the traditional sense. 

### Overview of Generative Models

Generative models follow to fit into two categories: Explicit and Implicit

<figure>
<img src="{{site.baseurl}}/images/post_im/model_tax.png">
  <figcaption>Generative Model Fitting Taxonomy from [2]</figcaption>
</figure>


### Maximum Likelihood Estimation (MLE) using an Explicit Density

Many generative models are trained using MLE with an explicit density function.  In particular, the model distribution $p\_\{\text{model}\}$ is selected from a specific parametric family of distributions with parameters $\theta$ (e.g. Gaussian, Bernoulli). Given a model distribution $p\_\{\text{model}\}\(x;\theta\)$ with parameters $\theta$ and an i.i.d dataset $\\{x_i\\}\_{i=1}^N$ sampled from $p\_{\text{data}}(x)$. The likelihood of the data under the given model is given by $\prod\_\{i=1\}^N p\_\{\text{model}\}(x\_i;\theta)$. ML estimation involves finding the parameters that maximize the likelihood of the data under the given model:

$$ \theta^* = \text{argmax}_\theta \prod_{i=1}^N p_{\text{model}}(x_i;\theta) $$

Since $0<a < b \implies \log a < \log b$,

$$ \theta^* = \text{argmax}_\theta \sum_{i=1}^N \log p_{\text{model}}(x_i;\theta)$$

Maximizing the ML is equivalent to minimizing the KL divergence between the $p\_{\text{data}}(x)$ and $p\_\{\text{model}\}\(x;\theta\)$:

$$ \text{argmax}_\theta \sum_{i=1}^N \log p_{\text{model}}(x_i;\theta)$$

$$ \text{argmin}_\theta -\frac{1}{N} \sum_{i=1}^N \log p_{\text{model}}(x_i;\theta)$$

$$ \text{argmin}_\theta -\frac{1}{N} \sum_{i=1}^N [\log p_{\text{model}}(x_i;\theta)  - \log p_{\text{data}}(x)]$$

Using the law of large numbers:

$$ \text{argmin}_\theta -\mathbb{E}_{p_{\text{data}}(x)}[\log p_{\text{model}}(x_i;\theta)  - \log p_{\text{data}}(x)]$$

$$ \text{argmin}_\theta -\mathbb{E}_{p_{\text{data}}(x)}\bigg[\log \frac{ p_{\text{model}}(x_i;\theta)}{ p_{\text{data}}(x)}\bigg]$$

In essence, the MLE attempts to find the distribution in the parametric family $p_{\text{model}}(x;\theta)$ that is the closest to the $p\_{\text{data}}(x)$. As suggested in [2], sometimes tractable explicit densities impose design limitations, so explicit models with intractabilities are used instead. Explicit models with intractabilities often require variational approximations (Variational Autoencoders) or Markov chain approximations (Restricted Boltzmann Machines).

GAN are not trained using MLE. Instead, GANs belong to a family of generative models that aren't fitted using explicit ML, and such models are called implict models.

### Implicit models

An implicit generative model samples from a latent variable from the distribution $p(\mathbf{z})$ where $\mathbf{z} \in \mathcal{Z} \subseteq \mathbb{R}^m$, and uses a deterministic function $G: \mathcal{Z} \mapsto \mathcal{X}$, in order to map the latent variable $\mathbf{z}$ to $\mathbf{x} \in \mathcal{X} \subseteq \mathbb{R}^n$. The generation process is shown below as graphical model:

<figure>
<img src="{{site.baseurl}}/images/post_im/implicit.png">
  <figcaption>Implicit Model as a Graphical Model</figcaption>
</figure>

The distribution induced by the transform $G$ is given by:

$$ p_g(x) = \frac{\partial}{\partial x_1} \cdots \frac{\partial}{\partial x_n} \int_{G(\mathbf{z}) \leq \mathbf{x}} p(\mathbf{z}) d\mathbf{z} $$

A common application of implicit models is the creation of random variables (e.g. Gaussian, Laplacian, Bernoulli) from uniform distributions. For example, given two independent uniform random variables $u_1 \sim U_1, u_2 \sim U_2$ on the interval $[0,1]$ and the deterministic transform $ x = \sqrt\{-2\log u_1 \} \cos(2 \pi u_2)$ yields a unit Gaussian random variable. Typically, the function $G$ is parameterized with a set of parameters $\theta$. In the deep learning setting, $G_\theta$ is a neural network model and $\theta$ are the network parameters. When $G_\theta$ is an neural network, computing $p_g(z)$ is intractable.  


# GANs

### Setup

Generative Adversarial Networks (GANs) provides a framework for estimating implicit generative models using an adversarial two-player game procedure.  A GAN consists of two neural networks: Generator ($G$) and Discriminator ($D$) with parameters $\theta_g$ and $\theta_d$ respectively.  The generator network is the deterministic function that attempts to map the input latent random vector with distribution $p(\mathbf{z})$ (called a noise vector) to the desired data distribution (called the real data distribution). The distribution given by the generator is denoted by $x = G(\mathbf{z}) \sim  p_g(\mathbf{x})$. The discriminator network has the task of determining the whether the data presented to it came from the generator network's output distribution or the real data distribution. The discriminator outputs a probability that indicates it's belief that the input came from the real data distribution. In essence, if $\mathbf{x} \sim p_{\text{data}}(\mathbf{x})$, $D(\mathbf{x}) \rightarrow 1$ and if $\mathbf{x} \sim p_{g}(\mathbf{x})$, $D(\mathbf{x}) \rightarrow 0$.  The generator network wins the game when the discriminator network is reduced to random guessing (e.g $D = 0.5$ for all input). A useful analogy from the original paper [1] describes the adversarial training process as: ”The generative model can be thought of as analogous to a team of counterfeiters, trying to produce fake currency and use it without detection, while the discriminative model is analogous to the police, trying to detect the counterfeit currency”.



### Training



### Toy Example 

#### Setup 

The $p_{\text{data}}$ data distribution equals a Gaussian $\mathcal{N}(3,1)$. We will assume our generator using the following noise prior $z\sim\mathcal{N}(0,1)$ and the generator has the following form $G(z;b) = z + b$. The discriminator has following form $D(x;[w,c]) = \sigma(wx + c)$ where $\sigma$ is a sigmoid. Here we initialize $b=2$ and $c=-2$ and $w$ is drawn from a $\mathcal{N}(0,1)$.

<figure>
 <video width="650" height="325" controls>
  <source src="{{site.baseurl}}/images/post_im/converge.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
<figcaption>Progression of the Generator and Discriminator during training</figcaption> 
</figure>

Check out the Colab notebook below to explore how to train the toy GAN:
<a href="https://colab.research.google.com/github/ceroytres/website_notebooks/blob/master/simple_gan.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a>


# Evaluation Metrics

Evaluation metrics are still an open research problem. 

### Parzen Density Estimation

Parzen Density Estimation using a Gaussian Kernel with bandwidth $\sigma$:

$$p_n(x) = \frac{1}{n}\sum_{i=1}^n \frac{1}{(2\pi\sigma^2)^{d/2}} \exp{\frac{1}{2\sigma^2}||x-x_i||^2}$$

where $x_i \in \mathbb{R}^d \sim p_g$ and $\sigma$ is determined using cross-validation. 
* The Parzen Density estimate attempts to estimate $p_g$. 
* The Parzen Density estimate is used to evaluate the likelihood of the test data under the estimated $p_g$. 
* Parzen density estimates tend to perform poorly in high dimensional spaces.

### Inception Score (IS)

$$ \text{IS} = \exp(\mathbb{E}_{x\sim p_g} KL(p(y|x)||p(y)) $$

Both $p(y\|x)$ and $p(y)= \int p(y\|x)p\_g(x) dx \approx \frac\{1\}\{n\}\sum\_i p(y\|x\_i)$ are estimated using an ImageNet pre-trained Inception v3. 

* $p(y\|x)$ should be low entropy if the objects produce a clear and yield confident classifications.
* $p(y)$ generated samples should have high diversity and therefore high entropy.

A higher IS implies better generative quality. IS has some drawbacks such a sensitivity to the Inception weights used, fails to detect memorization, and the need to estimate $p(y)$. See [10] for a detailed discussion on the IS.  

# References:
1. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, BingXu, David Warde-Farley, Sherjil Ozair, Aaron Courville,and Yoshua Bengio,Generative adversarial nets, Advances in neural information processing systems, 2014,pp. 2672–2680
2. Ian Goodfellow,Nips 2016 tutorial:  Generative adversarialnetworks, arXiv preprint arXiv:1701.00160 (2016)
2. Mohamed, Shakir, and Balaji Lakshminarayanan. "Learning in implicit generative models." arXiv preprint arXiv:1610.03483 (2016).
3. [Pieter Abbeel's CS294-158-SP20 Deep Unsupervised Learning Spring 2020 Notes](https://drive.google.com/open?id=1qCVpu2zFz1uEe3QcNHGlaT1Rs2u8HrCc)
2. Rockafellar, R. T. "Convex analysis in the calculus of variations." Advances in Convex Analysis and Global Optimization. Springer, Boston, MA, 2001. 135-151.
2. Salimans, Tim, et al. "Improved techniques for training gans." Advances in neural information processing systems. 2016.
10. Barratt, Shane, and Rishi Sharma. "A note on the inception score." arXiv preprint arXiv:1801.01973 (2018).